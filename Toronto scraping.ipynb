{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From zero to scraping\n",
    "\n",
    "Depending on how much time we have, this may end up being more of a demo than a full hands-on experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What am I even looking at right now?\n",
    "\n",
    "This is an IPython Notebook, which is basically just a fancy Python script that can be executed in chunks and annotated with helpful text. The aim is to make Python stuff more approachable and easily digestible. We'll see!\n",
    "\n",
    "(Barely important side note: Wakari is actually using an older version of this setup; they've rebranded as [Jupyter](http://jupyter.org/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, what do we need to know to make this happen?\n",
    "\n",
    "1. Python will execute your instructions one by one.\n",
    "2. Variables store things like strings, numbers and lists of items (like strings and numbers)\n",
    "3. We can extend Python's innate abilities with outside libraries designed for specific tasks.\n",
    "    - **What the hell does that mean?** It means smart people have written things that allow you to emulate a web browser or dissect complex HTML without much work.\n",
    "4. Files are typically read and/or written one line (row) at a time.\n",
    "5. Loops help you do the same thing to every item in a list.\n",
    "    - Like a list made up of rows in an online table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How will we scrape [this website](http://www.nrc.gov/reactors/operating/list-power-reactor-units.html)?\n",
    "\n",
    "1. We will import some libraries that:\n",
    "    - Act like an internet browser\n",
    "    - Parse HTML code\n",
    "    - Read and write CSV files\n",
    "2. Grab the contents of the web page.\n",
    "3. Parse the contents of the web page and target only the data table.\n",
    "4. Open a blank CSV file to store the information in the data table.\n",
    "5. Loop through each row in the online data table:\n",
    "    - Extract each element (cell) and store it in a variable\n",
    "    - Write those variables as a row into the CSV file\n",
    "6. Close the CSV file.\n",
    "7. Rejoice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why will we scrape this way?\n",
    "\n",
    "While code-free tools are handy in a pinch, scripts written in Python or another language are more flexible and adaptable. They can also run automatically in the background on a schedule. Also, you don't have to worry about a service or a tool ever disappearing, making all your hard work for naught."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries to do the heavy lifting\n",
    "\n",
    "We're going to bring in three outside modules to help us scrape this page.\n",
    "\n",
    "- **requests** will act like an internet browser and collect HTML\n",
    "- **BeautifulSoup** will parse the HTML code and allow us to isolate a data table\n",
    "- **csv** will allow us to write what we find to a nicely formatted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Grab the contents of a web page.\n",
    "\n",
    "The page we want is located here: http://www.nrc.gov/reactors/operating/list-power-reactor-units.html\n",
    "\n",
    "**requests** has a method called *get*, which is analagous to a browser like Firefox or Chrome fetching the HTML code for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check this quickly to see if we've gotten the expected raw HTML code by using another **requests** method that returns the HTML code as plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parse the HTML and target the table\n",
    "\n",
    "Now we can send our HTML code to **BeautifulSoup**, which is specifically designed to navigate the structural elements of the document, breaking off the pieces we choose. In this case, we are after the web page's only table -- it has all the data we need.\n",
    "\n",
    "**BeautifulSoup** has methods called *find* and *find_all* designed to target HTML tags. While *find* picks up the first matching instance, *find_all* locates all matching instances and returns them as a kind of list. We will use this to our advantage in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can check to see if we've isolated the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Open a blank CSV file for data storage\n",
    "\n",
    "We need a place for all this data to go once we start scraping it; we can open a new blank file and then use the **csv** method *writer* to create an object (stay with me now) that we can order around with some basic commands, making it write data to the new blank file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write our inaugural row to the file: the header that specifies what all the different columns are. We'll use **csv**'s *writerow* to send a list of what we would like written to the file: `\"NAME\", \"LINK\", \"DOCKET\", \"TYPE\", \"LOCATION\", \"OWNER\", \"REGION\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Loop through each row in the table, extract data and write it to the file\n",
    "\n",
    "Here comes the tricky part: we have to actually scrape the data out of the table we isolated.\n",
    "\n",
    "To do that, we need to not only loop through every row in the table, but also each cell in every row.\n",
    "\n",
    "The basic nitty-gritty of Python can be self-explanatory to a certain extent, but loops tend to hang people up who haven't been exposed to the concept before.\n",
    "\n",
    "A loop just does the same thing to every item in a list. It's a very helpful structure for scraping, because you can essentially treat a table like a list of rows.\n",
    "\n",
    "Let's experiment for a minute on an example list:\n",
    "\n",
    "```\n",
    "my_list = ['Toronto', 'Ontario', 2016, 'May']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to do something to each item in this list without having to retype it repeatedly. This basic syntax, in pseudocode:\n",
    "\n",
    "```\n",
    "for [a list item] in [some list]:\n",
    "    do a thing with [a list item]\n",
    "```\n",
    "\n",
    "That thing will then happen with the first list item, the second, the third, etc., until the end of the list is reached.\n",
    "\n",
    "So if I wanted to print each thing in the list we made above, one by one, I could do it like this:\n",
    "\n",
    "```\n",
    "for thing in my_list:\n",
    "    print(thing)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing special about the variable name `thing`; all it does is hold the list item until the loop moves on to the next. We could call it `banana` or `zorro` and the result would be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, you're seeing the last list item that was passed into the loop variable.\n",
    "\n",
    "So now we have to dive into the table with this long-ish list of steps. We'll make a list of HTML snippets wrapped in `<tr>` tags (the table rows), and then a list within that of the actual data cells inside each `<td>`.\n",
    "\n",
    "```\n",
    "for row in reactor_table.find_all('tr')[1:]:\n",
    "\n",
    "    # Each <tr> tag also has some <td> tags holding cell contents; these are\n",
    "    # what we'll move into variables and then write to the CSV file.\n",
    "    cell = row.find_all('td')\n",
    "    \n",
    "    # Reactor name, detail page link and docket number are all part of the first cell.\n",
    "    # Docket has a bunch of whitespace, so we'll .strip() it.\n",
    "    name = cell[0].contents[0].text\n",
    "    link = cell[0].contents[0].get('href')\n",
    "    docket = cell[0].contents[2].strip()\n",
    "    reactype = cell[1].text\n",
    "    \n",
    "    # Two fields in this table (location and owner) have characters outside\n",
    "    # of our fair ASCII realm; we need to make sure these are encoded into a\n",
    "    # character system (and one that can handle them) on the way into our CSV.\n",
    "    # We'll put them in UTF-8, the original encoding of our page.\n",
    "    location = cell[2].text.encode('utf-8')\n",
    "    owner = cell[3].text.strip().encode('utf-8')\n",
    "    region = cell[4].text\n",
    "\n",
    "    # Once everything's collected, write it as a row in the csv.\n",
    "    output.writerow([name, link, docket, reactype, location, owner, region])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop has done all the work! Just one thing left to do:\n",
    "\n",
    "### 6. Close the file\n",
    "\n",
    "Some of it just hangs out in the computer's memory until you close the file and commit it all to disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
